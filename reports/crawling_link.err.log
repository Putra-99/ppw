Traceback (most recent call last):
  File "C:\laragon\www\ppw-1\venv\lib\site-packages\jupyter_cache\executors\utils.py", line 58, in single_nb_execution
    executenb(
  File "C:\laragon\www\ppw-1\venv\lib\site-packages\nbclient\client.py", line 1305, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "C:\laragon\www\ppw-1\venv\lib\site-packages\jupyter_core\utils\__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
  File "C:\laragon\bin\python\python-3.10\lib\asyncio\base_events.py", line 646, in run_until_complete
    return future.result()
  File "C:\laragon\www\ppw-1\venv\lib\site-packages\nbclient\client.py", line 705, in async_execute
    await self.async_execute_cell(
  File "C:\laragon\www\ppw-1\venv\lib\site-packages\nbclient\client.py", line 1058, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "C:\laragon\www\ppw-1\venv\lib\site-packages\nbclient\client.py", line 914, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import pandas as pd
import requests, time
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def crawl_recursive(start_url, max_depth=2, delay=1):
    """
    Crawl secara rekursif:
    start_url: URL awal
    max_depth: kedalaman maksimum (1 = hanya halaman awal, 2 = halaman awal + link di dalamnya, dst)
    delay: jeda antar request

    Return DataFrame dengan kolom: id, page, link
    """
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})

    visited = set()  # URL yang sudah dikunjungi
    results = []
    link_id = 1

    # Queue untuk BFS: (url, depth, parent_url)
    queue = [(start_url, 0, None)]

    while queue:
        current_url, depth, parent = queue.pop(0)
        if current_url in visited:
            continue
        visited.add(current_url)

        # Ambil halaman
        try:
            r = session.get(current_url, timeout=15)
            r.raise_for_status()
        except Exception as e:
            print(f"Gagal akses {current_url}: {e}")
            continue

        soup = BeautifulSoup(r.content, "html.parser")
        all_links = soup.find_all("a", href=True)

        # Simpan setiap link ditemukan
        for a in all_links:
            href = a["href"]
            absolute_link = urljoin(current_url, href)  # jadi URL penuh

            results.append({
                "id": link_id,
                "page": current_url,  # halaman asal
                "link": absolute_link  # link ditemukan di halaman ini
            })
            link_id += 1

            # Jika belum mencapai max_depth, tambahkan ke queue untuk di-crawl lagi
            if depth + 1 < max_depth:
                queue.append((absolute_link, depth + 1, current_url))

        time.sleep(delay)  # jeda agar tidak overload server

    df = pd.DataFrame(results)
    return df

# Contoh penggunaan:
df_crawl = crawl_recursive("https://pta.trunojoyo.ac.id/", max_depth=2)
print(df_crawl.head())

# Simpan ke CSV
df_crawl.to_csv("recursive_crawl.csv", index=False, encoding="utf-8-sig")
print("Hasil tersimpan ke recursive_crawl.csv")

------------------


[1;31m---------------------------------------------------------------------------[0m
[1;31mModuleNotFoundError[0m                       Traceback (most recent call last)
Cell [1;32mIn[3], line 1[0m
[1;32m----> 1[0m [38;5;28;01mimport[39;00m[38;5;250m [39m[38;5;21;01mpandas[39;00m[38;5;250m [39m[38;5;28;01mas[39;00m[38;5;250m [39m[38;5;21;01mpd[39;00m
[0;32m      2[0m [38;5;28;01mimport[39;00m[38;5;250m [39m[38;5;21;01mrequests[39;00m[38;5;241m,[39m[38;5;250m [39m[38;5;21;01mtime[39;00m
[0;32m      3[0m [38;5;28;01mfrom[39;00m[38;5;250m [39m[38;5;21;01mbs4[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m BeautifulSoup

[1;31mModuleNotFoundError[0m: No module named 'pandas'

